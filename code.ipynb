{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import json\n",
    "from transformers import BertTokenizer, BertModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dir = '/kaggle/input/dataset/data/' if os.path.exists('/kaggle') else './data/'\n",
    "local_dir = './'\n",
    "out_dir = '/kaggle/working/' if os.path.exists('/kaggle') else './'\n",
    "batch_size = 32\n",
    "num_epochs = 60\n",
    "\n",
    "model_to_train = 0\n",
    "load = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from test.json, train.json, dev.json using DataLoader\n",
    "\n",
    "class seqDataset(Dataset):\n",
    "    def __init__(self, path, mode, in_vocab = {}, out_vocab = {}):\n",
    "        self.path = path\n",
    "        self.mode = mode\n",
    "        self.Problem = []\n",
    "        self.linear_formula = []\n",
    "        self.answer = []\n",
    "        self.load_data()\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.path + self.mode + '.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "            for i in range(len(data)):\n",
    "                x = (data[i]['Problem'] + ' <eos>').split()\n",
    "                n = 0\n",
    "                for j in range(len(x)):\n",
    "                    try:\n",
    "                        float(x[j])\n",
    "                    except:\n",
    "                        continue\n",
    "                    x[j] = 'n' + str(n)\n",
    "                    n += 1\n",
    "                self.Problem.append(x)\n",
    "                # self.Problem.append((data[i]['Problem'] + ' <eos>').split())\n",
    "                self.linear_formula.append((data[i]['linear_formula'] + '|<eos>').replace(' ', '|').replace('(','|').replace(')', '|').replace(',', '|').replace('|', ' ').split())\n",
    "                self.answer.append(data[i]['answer'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Problem)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.Problem[idx]), (self.linear_formula[idx]), self.answer[idx]\n",
    "    \n",
    "    def print(self, idx):\n",
    "        print(' '.join(self.Problem[idx]), '|'.join(self.linear_formula[idx]), self.answer[idx], sep='\\n')\n",
    "    \n",
    "\n",
    "def collate_fn(batch):\n",
    "    Problem, linear_formula, answer = zip(*batch)\n",
    "    Problem = pad_sequence(Problem, batch_first=True, padding_value=0)\n",
    "    linear_formula = pad_sequence(linear_formula, batch_first=True, padding_value=0)\n",
    "    # answer = torch.tensor(answer, dtype=torch.float32)\n",
    "    return Problem, linear_formula, answer\n",
    "\n",
    "train_dataset = seqDataset(dir, 'train')\n",
    "test_dataset = seqDataset(dir, 'test')\n",
    "dev_dataset = seqDataset(dir, 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vocab = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# print(bert_vocab.vocab['[PAD]'])\n",
    "# something = [' '.join(sentence[:-1]) for sentence in train_dataset.Problem[:32]]\n",
    "# something_else = bert_vocab(something, padding=True, return_tensors='pt')\n",
    "# print(something)\n",
    "# print(something_else['input_ids'])\n",
    "# print(something_else['attention_mask'])\n",
    "\n",
    "def bert_collate(batch):\n",
    "    Problem, linear_formula, answer = zip(*batch)\n",
    "    Problem = [' '.join(sentence[:-1]) for sentence in Problem]\n",
    "    Problem = bert_vocab(Problem, padding=True, return_tensors='pt')\n",
    "    linear_formula = pad_sequence(linear_formula, batch_first=True, padding_value=0)\n",
    "    return Problem, linear_formula, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "hidden_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GloVe = torchtext.vocab.GloVe(name='6B', dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_glove_embedding(model, model_vocab, train_sentences):\n",
    "    vocab = {'<pad>':0, '<start>':1, '<eos>':2, '<unknown>':3}\n",
    "    i = 4\n",
    "    for sentence in train_sentences:\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = i\n",
    "                i += 1\n",
    "\n",
    "    embedding = torch.FloatTensor(len(vocab), embedding_dim).uniform_(-0.25, 0.25)\n",
    "\n",
    "    for word in vocab:\n",
    "        if word in model_vocab:\n",
    "            embedding[vocab[word]] = model.vectors[model_vocab[word]]\n",
    "\n",
    "    return embedding, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output_vocab(train_out_sentences):\n",
    "    vocab = {'<pad>':0, '<start>':1, '<eos>':2, '<unknown>':3}\n",
    "    rev_vocab = {0:'<pad>', 1:'<start>', 2:'<eos>', 3:'<unknown>'}\n",
    "    i = 4\n",
    "    for sentence in train_out_sentences:\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = i\n",
    "                rev_vocab[i] = word\n",
    "                i += 1\n",
    "    \n",
    "    return vocab, rev_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_dataset(vocab, data):    \n",
    "    for i in range(len(data)):\n",
    "        data[i] = torch.tensor([vocab[word] if word in vocab else vocab['<unknown>'] for word in data[i]])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, vocab = make_glove_embedding(GloVe, GloVe.stoi, train_dataset.Problem)\n",
    "out_vocab, rev_out_vocab = make_output_vocab(train_dataset.linear_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(model_to_train == 0 or model_to_train == 1):\n",
    "    train_dataset.Problem = modify_dataset(vocab, train_dataset.Problem)\n",
    "    test_dataset.Problem = modify_dataset(vocab, test_dataset.Problem)\n",
    "    dev_dataset.Problem = modify_dataset(vocab, dev_dataset.Problem)\n",
    "\n",
    "    train_dataset.linear_formula = modify_dataset(out_vocab, train_dataset.linear_formula)\n",
    "    test_dataset.linear_formula = modify_dataset(out_vocab, test_dataset.linear_formula)\n",
    "    dev_dataset.linear_formula = modify_dataset(out_vocab, dev_dataset.linear_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_vocab = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "if(model_to_train == 2 or model_to_train==3):\n",
    "    print('Loading BERT dataset')\n",
    "    # train_dataset.Problem = [torch.tensor(bert_vocab.encode(' '.join(sentence))) for sentence in train_dataset.Problem]\n",
    "    # test_dataset.Problem = [torch.tensor(bert_vocab.encode(' '.join(sentence))) for sentence in test_dataset.Problem]\n",
    "    # dev_dataset.Problem = [torch.tensor(bert_vocab.encode(' '.join(sentence))) for sentence in dev_dataset.Problem]\n",
    "\n",
    "    train_dataset.linear_formula = modify_dataset(out_vocab, train_dataset.linear_formula)\n",
    "    test_dataset.linear_formula = modify_dataset(out_vocab, test_dataset.linear_formula)\n",
    "    dev_dataset.linear_formula = modify_dataset(out_vocab, dev_dataset.linear_formula)\n",
    "\n",
    "    vocab = bert_vocab.vocab\n",
    "    collator = bert_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collator)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collator)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True, collate_fn=collator)\n",
    "\n",
    "train_acc_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=collator)\n",
    "test_acc_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collator)\n",
    "dev_acc_loader = DataLoader(dev_dataset, batch_size=1, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class beamdata:\n",
    "    def __init__(self, score, sequence, hidden, cell):\n",
    "        self.score = score\n",
    "        self.sequence = sequence\n",
    "        self.hidden = hidden\n",
    "        self.cell = cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 1\n",
    "# seq2seq\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding = embedding, hidden_dim = hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding, padding_idx=0)\n",
    "        self.embedding_dim = self.embedding.embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.linear_h = nn.Linear(self.hidden_dim*2, self.hidden_dim)\n",
    "        self.linear_c = nn.Linear(self.hidden_dim*2, self.hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        x, (h, c) = self.lstm(x)\n",
    "        h = torch.tanh(self.linear_h(h.permute(1, 0, 2).reshape(x.size(0), -1)))\n",
    "        c = torch.tanh(self.linear_c(c.permute(1, 0, 2).reshape(x.size(0), -1)))\n",
    "        return x, h, c\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_vocab = out_vocab, hidden_dim = hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(out_vocab), embedding_dim, padding_idx=0)\n",
    "        self.embedding_dim = self.embedding.embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_dim, len(out_vocab))\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        h = h.unsqueeze(0)\n",
    "        c = c.unsqueeze(0)\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        x = x.unsqueeze(1)\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        x = self.linear(x.squeeze(1))\n",
    "        h = h.squeeze(0)\n",
    "        c = c.squeeze(0)\n",
    "        return x, h, c\n",
    "\n",
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, teacher_forcing_ratio = 0.6, beam_size = 10, out_vocab = out_vocab):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.beam_size = beam_size\n",
    "        self.out_vocab = out_vocab\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        if(self.training):\n",
    "            return self.train_forward(x, y)\n",
    "        else:\n",
    "            return self.test_forward_beam(x, y)\n",
    "        \n",
    "    def train_forward(self, x, y):\n",
    "        _, h, c = self.encoder(x)\n",
    "        out = out_vocab['<start>']*torch.ones(x.size(0), dtype = torch.long).to(device)\n",
    "        outputs = torch.zeros(y.size(0), y.size(1), len(out_vocab)).to(device)\n",
    "        for i in range(y.size(1)):\n",
    "            if(np.random.random() < self.teacher_forcing_ratio):\n",
    "                if(i != 0):\n",
    "                    out = y[:, i-1]\n",
    "            else:\n",
    "                if(i != 0):\n",
    "                    out = torch.argmax(out, dim=1)\n",
    "            out, h, c = self.decoder(out, h, c)\n",
    "            outputs[:, i] = out\n",
    "        return outputs\n",
    "    \n",
    "    def test_forward_beam(self, x, y):\n",
    "        _, h, c = self.encoder(x)\n",
    "        out = out_vocab['<start>']*torch.ones(x.size(0), dtype = torch.long).to(device)\n",
    "        beam = [beamdata(0, [out], h, c)]\n",
    "        for i in range(y.size(1)):\n",
    "            new_beam = []\n",
    "            for b in beam:\n",
    "                out, h, c = self.decoder(torch.tensor(b.sequence[-1]), b.hidden, b.cell)\n",
    "                out = F.log_softmax(out, dim=1)\n",
    "                out = torch.topk(out, self.beam_size, dim=1)\n",
    "                for j in range(self.beam_size):\n",
    "                    new_beam.append(beamdata(b.score + out[0][:,j], b.sequence + [out[1][:,j]], h, c))\n",
    "\n",
    "            beam = sorted(new_beam, key=lambda x: x.score.sum(), reverse=True)[:self.beam_size]\n",
    "\n",
    "        sequence = torch.zeros(y.size(0), y.size(1), dtype=torch.long).to(device)\n",
    "        for i in range(y.size(1)):\n",
    "            sequence[:, i] = torch.tensor(beam[0].sequence[i])\n",
    "\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 2\n",
    "# AttnSeq2seq\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attn = nn.Linear(self.hidden_dim*3, self.hidden_dim)\n",
    "        self.v = nn.Linear(self.hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden):\n",
    "        hidden = hidden.unsqueeze(1)\n",
    "        hidden = hidden.repeat(1, encoder_outputs.size(1), 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), 2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, out_vocab = out_vocab, hidden_dim = hidden_dim):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(out_vocab), embedding_dim, padding_idx=0)\n",
    "        self.embedding_dim = self.embedding.embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim + 2*self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_dim, len(out_vocab))\n",
    "        self.attention = Attention(self.hidden_dim)\n",
    "\n",
    "    def forward(self, x, h, c, encoder_outputs):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        attention_weights = self.attention(encoder_outputs, h)\n",
    "        attention_weights = attention_weights.unsqueeze(1)\n",
    "        context = attention_weights.bmm(encoder_outputs)\n",
    "        x = torch.cat((x, context), dim=2)\n",
    "        x, (h, c) = self.lstm(x, (h.unsqueeze(0), c.unsqueeze(0)))\n",
    "        x = self.linear(x.squeeze(1))\n",
    "        h = h.squeeze(0)\n",
    "        c = c.squeeze(0)\n",
    "        return x, h, c\n",
    "    \n",
    "\n",
    "class AttnSeq2seq(nn.Module):\n",
    "    def __init__(self, teacher_forcing_ratio = 0.6, beam_size = 10, out_vocab = out_vocab):\n",
    "        super(AttnSeq2seq, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = AttnDecoder()\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.beam_size = beam_size\n",
    "        self.out_vocab = out_vocab\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        if(self.training):\n",
    "            return self.train_forward(x, y)\n",
    "        else:\n",
    "            return self.test_forward_beam(x, y)\n",
    "        \n",
    "    def train_forward(self, x, y):\n",
    "        encoder_outputs, h, c = self.encoder(x)\n",
    "        out = out_vocab['<start>']*torch.ones(x.size(0), dtype = torch.long).to(device)\n",
    "        outputs = torch.zeros(y.size(0), y.size(1), len(out_vocab)).to(device)\n",
    "        for i in range(y.size(1)):\n",
    "            if(np.random.random() < self.teacher_forcing_ratio):\n",
    "                if(i != 0):\n",
    "                    out = y[:, i-1]\n",
    "            else:\n",
    "                if(i != 0):\n",
    "                    out = torch.argmax(out, dim=1)\n",
    "            out, h, c = self.decoder(out, h, c, encoder_outputs)\n",
    "            outputs[:, i] = out\n",
    "        return outputs\n",
    "    \n",
    "    def test_forward_beam(self, x, y):\n",
    "        encoder_outputs, h, c = self.encoder(x)\n",
    "        out = out_vocab['<start>']*torch.ones(x.size(0), dtype = torch.long).to(device)\n",
    "        beam = [beamdata(0, [out], h, c)]\n",
    "        for i in range(y.size(1)):\n",
    "            new_beam = []\n",
    "            for b in beam:\n",
    "                out, h, c = self.decoder(torch.tensor(b.sequence[-1]), b.hidden, b.cell, encoder_outputs)\n",
    "                out = F.log_softmax(out, dim=1)\n",
    "                out = torch.topk(out, self.beam_size, dim=1)\n",
    "                for j in range(self.beam_size):\n",
    "                    new_beam.append(beamdata(b.score + out[0][:,j], b.sequence + [out[1][:,j]], h, c))\n",
    "\n",
    "            beam = sorted(new_beam, key=lambda x: x.score.sum(), reverse=True)[:self.beam_size]\n",
    "\n",
    "        sequence = torch.zeros(y.size(0), y.size(1), dtype=torch.long).to(device)\n",
    "        for i in range(y.size(1)):\n",
    "            sequence[:, i] = torch.tensor(beam[0].sequence[i])\n",
    "\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_frozen = BertModel.from_pretrained('bert-base-uncased')\n",
    "BERT_fine_tuned = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "for param in BERT_frozen.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in BERT_fine_tuned.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in BERT_fine_tuned.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models 3 and 4\n",
    "# BertSeq2seq\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attn = nn.Linear(self.hidden_dim*4, self.hidden_dim)\n",
    "        self.v = nn.Linear(self.hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden):\n",
    "        hidden = hidden.unsqueeze(1)\n",
    "        hidden = hidden.repeat(1, encoder_outputs.size(1), 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), 2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, BERT_model, hidden_dim = hidden_dim):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.bert = BERT_model\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.linear_h = nn.Linear(768, self.hidden_dim)\n",
    "        self.linear_c = nn.Linear(768, self.hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(x['input_ids'], attention_mask=x['attention_mask']).last_hidden_state\n",
    "        h = torch.tanh(self.linear_h(x[:,0]))\n",
    "        c = torch.tanh(self.linear_c(x[:,0]))\n",
    "        return x, h, c\n",
    "    \n",
    "class BertDecoder(nn.Module):\n",
    "    def __init__(self, out_vocab = out_vocab, hidden_dim = hidden_dim):\n",
    "        super(BertDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(out_vocab), embedding_dim, padding_idx=0)\n",
    "        self.embedding_dim = self.embedding.embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim + 3*self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_dim, len(out_vocab))\n",
    "        self.attention = BertAttention(self.hidden_dim)\n",
    "\n",
    "    def forward(self, x, h, c, encoder_outputs):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        attention_weights = self.attention(encoder_outputs, h)\n",
    "        attention_weights = attention_weights.unsqueeze(1)\n",
    "        context = attention_weights.bmm(encoder_outputs)\n",
    "        x = torch.cat((x, context), dim=2)\n",
    "        x, (h, c) = self.lstm(x, (h.unsqueeze(0), c.unsqueeze(0)))\n",
    "        x = self.linear(x.squeeze(1))\n",
    "        h = h.squeeze(0)\n",
    "        c = c.squeeze(0)\n",
    "        return x, h, c\n",
    "    \n",
    "class BertSeq2seq(nn.Module):\n",
    "    def __init__(self, BERT_model, teacher_forcing_ratio = 0.6, beam_size = 10, out_vocab = out_vocab):\n",
    "        super(BertSeq2seq, self).__init__()\n",
    "        self.encoder = BertEncoder(BERT_model)\n",
    "        self.decoder = BertDecoder()\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.beam_size = beam_size\n",
    "        self.out_vocab = out_vocab\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        if(self.training):\n",
    "            return self.train_forward(x, y)\n",
    "        else:\n",
    "            return self.test_forward_beam(x, y)\n",
    "        \n",
    "    def train_forward(self, x, y):\n",
    "        encoder_outputs, h, c = self.encoder(x)\n",
    "        out = out_vocab['<start>']*torch.ones(y.size(0), dtype = torch.long).to(device)\n",
    "        outputs = torch.zeros(y.size(0), y.size(1), len(out_vocab)).to(device)\n",
    "        for i in range(y.size(1)):\n",
    "            if(np.random.random() < self.teacher_forcing_ratio):\n",
    "                if(i != 0):\n",
    "                    out = y[:, i-1]\n",
    "            else:\n",
    "                if(i != 0):\n",
    "                    out = torch.argmax(out, dim=1)\n",
    "            out, h, c = self.decoder(out, h, c, encoder_outputs)\n",
    "            outputs[:, i] = out\n",
    "        return outputs\n",
    "    \n",
    "    def test_forward_beam(self, x, y):\n",
    "        # print(self.beam_size)\n",
    "        encoder_outputs, h, c = self.encoder(x)\n",
    "        out = out_vocab['<start>']*torch.ones(y.size(0), dtype = torch.long).to(device)\n",
    "        beam = [beamdata(0, [out], h, c)]\n",
    "        for i in range(y.size(1)):\n",
    "            new_beam = []\n",
    "            for b in beam:\n",
    "                out, h, c = self.decoder(torch.tensor(b.sequence[-1]), b.hidden, b.cell, encoder_outputs)\n",
    "                out = F.log_softmax(out, dim=1)\n",
    "                out = torch.topk(out, self.beam_size, dim=1)\n",
    "                for j in range(self.beam_size):\n",
    "                    new_beam.append(beamdata(b.score + out[0][:,j], b.sequence + [out[1][:,j]], h, c))\n",
    "\n",
    "            beam = sorted(new_beam, key=lambda x: x.score.sum(), reverse=True)[:self.beam_size]\n",
    "\n",
    "        sequence = torch.zeros(y.size(0), y.size(1), dtype=torch.long).to(device)\n",
    "        for i in range(y.size(1)):\n",
    "            sequence[:, i] = torch.tensor(beam[0].sequence[i])\n",
    "\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(S, optimizer, criterion, num_epochs = 60, model_no  = '_-_'):\n",
    "    WHILE_TRAINING_STATISTICS = pd.DataFrame(columns = ['Epoch', 'Loss'])\n",
    "    WHILE_TRAINING_VALIDATION_STATISTICS = pd.DataFrame(columns = ['Epoch', 'Loss'])\n",
    "\n",
    "    for n in range(num_epochs):\n",
    "        S.train()\n",
    "        for i, (p, l, a) in enumerate(train_loader):\n",
    "            p = p.to(device)\n",
    "            l = l.to(device)\n",
    "            # a = a.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = S(p, l)\n",
    "            loss = criterion(output.view(-1, len(out_vocab)), l.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if(i%100 == 0):\n",
    "                print('Epoch:', n, 'Batch:', i, 'Loss:', loss.item())\n",
    "\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (p, l, a) in enumerate(train_acc_loader):\n",
    "                p = p.to(device)\n",
    "                l = l.to(device)\n",
    "                # a = a.to(device)\n",
    "                output = S(p, l)\n",
    "                loss = criterion(output.view(-1, len(out_vocab)), l.view(-1))\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            for i, (p, l, a) in enumerate(dev_acc_loader):\n",
    "                p = p.to(device)\n",
    "                l = l.to(device)\n",
    "                # a = a.to(device)\n",
    "                output = S(p, l)\n",
    "                loss = criterion(output.view(-1, len(out_vocab)), l.view(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(dev_loader)\n",
    "        print('*Epoch:', n, 'Train Loss:', train_loss)\n",
    "        print('*Epoch:', n, 'Validation Loss:', val_loss)\n",
    "\n",
    "        WHILE_TRAINING_STATISTICS.loc[len(WHILE_TRAINING_STATISTICS)] = [n, loss.item()]\n",
    "        WHILE_TRAINING_VALIDATION_STATISTICS.loc[len(WHILE_TRAINING_VALIDATION_STATISTICS)] = [n, val_loss]\n",
    "\n",
    "        WHILE_TRAINING_STATISTICS.to_csv(out_dir + model_no + '_train_statistics.csv', index=False)\n",
    "        WHILE_TRAINING_VALIDATION_STATISTICS.to_csv(out_dir + model_no + '_validation_statistics.csv', index=False)\n",
    "\n",
    "        torch.save(S.state_dict(), out_dir + model_no + '_seq2seq.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_prediction(mode, loader, S, model_no  = '_-_'):\n",
    "    df = 0\n",
    "    with open(dir + mode + '.json', 'r') as f:\n",
    "        df = json.load(f)\n",
    "\n",
    "    df_copy = pd.DataFrame(df).copy()\n",
    "    answers = []\n",
    "\n",
    "    S.eval()\n",
    "    for i, (p, l, a) in enumerate(loader):\n",
    "        p = p.to(device)\n",
    "        l = l.to(device)\n",
    "        # a = a.to(device)\n",
    "        output = S(p, l)\n",
    "\n",
    "        lst = []\n",
    "        for j in range(output.size(1)):\n",
    "            lst.append(rev_out_vocab[output[0][j].item()])\n",
    "\n",
    "        ans = \",\"\n",
    "        # print(lst)\n",
    "        for j in range(1,len(lst)):\n",
    "            if(lst[j] == '<eos>'):\n",
    "                break\n",
    "            if(lst[j][-1].isdigit()):\n",
    "                ans += lst[j] + ','\n",
    "            else:\n",
    "                ans = ans[:-1] + ')|' + lst[j] + '('\n",
    "\n",
    "        ans = ans[2:-1] + ')|'\n",
    "        answers.append(ans)\n",
    "\n",
    "    \n",
    "\n",
    "    df_copy['predicted'] = answers\n",
    "    \n",
    "    df_copy = df_copy[['Problem', 'answer', 'predicted', 'linear_formula']]\n",
    "    json_data = df_copy.to_json(orient='records')\n",
    "    parsed_data = json.loads(json_data)\n",
    "    script = dir + 'evaluator.py'\n",
    "    pred_file = out_dir + model_no + '_' + mode + '_predicted.json'\n",
    "    with open(pred_file, 'w') as f:\n",
    "        json.dump(parsed_data, f)\n",
    "\n",
    "    os.system('python3 ' + script + ' ' + pred_file)\n",
    "\n",
    "    exact_match = 0\n",
    "    exec_acc = 0\n",
    "    total = 0\n",
    "    with open(pred_file, 'r') as f:\n",
    "        final_json = json.load(f)\n",
    "        for i in range(len(final_json)):\n",
    "            if((final_json[i]['linear_formula'] == final_json[i]['predicted']) or ((len(final_json[i]['predicted']) > 0) and (final_json[i]['linear_formula'] == final_json[i]['predicted'][:-1]))):\n",
    "                exact_match += 1\n",
    "            if((final_json[i]['predicted_answer'] is not None) and (abs(final_json[i]['answer'] - final_json[i]['predicted_answer']) <= abs(final_json[i]['answer'])*0.02)):\n",
    "                exec_acc += 1\n",
    "            total += 1\n",
    "        \n",
    "    print('Exact Match:', 100*exact_match/total)\n",
    "    print('Execution Accuracy:', 100*exec_acc/total)\n",
    "\n",
    "    return exact_match, exec_acc, total\n",
    "\n",
    "def accuracy(S, model_no  = '_-_'):\n",
    "    ACC_DF = pd.DataFrame(columns = ['mode', 'exact_match', 'exec_acc'])\n",
    "\n",
    "    for mode in ['test', 'dev']:\n",
    "        exact_match, exec_acc, total = beam_search_prediction(mode, eval(mode + '_acc_loader'), S, model_no)\n",
    "        ACC_DF.loc[len(ACC_DF)] = [mode, (100 * exact_match)/total, (100 * exec_acc)/total]\n",
    "\n",
    "    ACC_DF.to_csv(out_dir + model_no + '_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "S = seq2seq()\n",
    "S.to(device)\n",
    "S = nn.DataParallel(S)\n",
    "optimizer_S = optim.Adam(S.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=out_vocab['<pad>'])\n",
    "\n",
    "Att_S = AttnSeq2seq()\n",
    "Att_S.to(device)\n",
    "Att_S = nn.DataParallel(Att_S)\n",
    "optimizer_Att_S = optim.Adam(Att_S.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=out_vocab['<pad>'])\n",
    "\n",
    "Bert_S_frozen = BertSeq2seq(BERT_frozen)\n",
    "Bert_S_frozen.to(device)\n",
    "Bert_S_frozen = nn.DataParallel(Bert_S_frozen)\n",
    "optimizer_Bert_S_frozen = optim.Adam(Bert_S_frozen.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=out_vocab['<pad>'])\n",
    "\n",
    "Bert_S_fine_tuned = BertSeq2seq(BERT_fine_tuned)\n",
    "Bert_S_fine_tuned.to(device)\n",
    "Bert_S_fine_tuned = nn.DataParallel(Bert_S_fine_tuned)\n",
    "optimizer_Bert_S_fine_tuned = optim.Adam(Bert_S_fine_tuned.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=out_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_load = seq2seq()\n",
    "Att_S_load = AttnSeq2seq()\n",
    "Bert_S_frozen_load = BertSeq2seq(BERT_frozen)\n",
    "Bert_S_fine_tuned_load = BertSeq2seq(BERT_fine_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(os.path.exists(local_dir + '0_seq2seq.pth')):\n",
    "    state_dict = torch.load(local_dir + '0_seq2seq.pth', map_location=torch.device('cpu'))\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith('module.'):\n",
    "            new_key = key[7:]  # Remove the \"module.\" prefix\n",
    "            new_state_dict[new_key] = value\n",
    "        else:\n",
    "            new_state_dict[key] = value\n",
    "\n",
    "    S_load.load_state_dict(new_state_dict)\n",
    "    S_load.to(device)\n",
    "    S_load = nn.DataParallel(S_load)\n",
    "\n",
    "\n",
    "if(os.path.exists(local_dir + '1_seq2seq.pth')):\n",
    "    state_dict = torch.load(local_dir + '1_seq2seq.pth', map_location=torch.device('cpu'))\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith('module.'):\n",
    "            new_key = key[7:]  # Remove the \"module.\" prefix\n",
    "            new_state_dict[new_key] = value\n",
    "        else:\n",
    "            new_state_dict[key] = value\n",
    "\n",
    "    Att_S_load.load_state_dict(new_state_dict)\n",
    "    Att_S_load.to(device)\n",
    "    Att_S_load = nn.DataParallel(Att_S_load)\n",
    "\n",
    "\n",
    "if(os.path.exists(local_dir + '2_seq2seq.pth')):\n",
    "    state_dict = torch.load(local_dir + '2_seq2seq.pth', map_location=torch.device('cpu'))\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith('module.'):\n",
    "            new_key = key[7:]  # Remove the \"module.\" prefix\n",
    "            new_state_dict[new_key] = value\n",
    "        else:\n",
    "            new_state_dict[key] = value\n",
    "\n",
    "    Bert_S_frozen_load.load_state_dict(new_state_dict)\n",
    "    Bert_S_frozen_load.to(device)\n",
    "    Bert_S_frozen_load = nn.DataParallel(Bert_S_frozen_load)\n",
    "\n",
    "\n",
    "if(os.path.exists(local_dir + '3_seq2seq.pth')):\n",
    "    state_dict = torch.load(local_dir + '3_seq2seq.pth', map_location=torch.device('cpu'))\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith('module.'):\n",
    "            new_key = key[7:]  # Remove the \"module.\" prefix\n",
    "            new_state_dict[new_key] = value\n",
    "        else:\n",
    "            new_state_dict[key] = value\n",
    "\n",
    "    Bert_S_fine_tuned_load.load_state_dict(new_state_dict)\n",
    "    Bert_S_fine_tuned_load.to(device)\n",
    "    Bert_S_fine_tuned_load = nn.DataParallel(Bert_S_fine_tuned_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [S, Att_S, Bert_S_frozen, Bert_S_fine_tuned]\n",
    "optimizers = [optimizer_S, optimizer_Att_S, optimizer_Bert_S_frozen, optimizer_Bert_S_fine_tuned]\n",
    "load_models = [S_load, Att_S_load, Bert_S_frozen_load, Bert_S_fine_tuned_load]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_sizes = [1, 10, 20]\n",
    "# tf_ratios = [0.3, 0.9]\n",
    "if not load:\n",
    "    # print(models[model_to_train].module.teacher_forcing_ratio)\n",
    "    train(models[model_to_train], optimizers[model_to_train], criterion, num_epochs, str(model_to_train))\n",
    "    # for bm_sz in beam_sizes:\n",
    "    #     models[model_to_train].module.beam_size = bm_sz\n",
    "    #     accuracy(models[model_to_train], str(model_to_train) + '_beam_' + str(bm_sz))\n",
    "    accuracy(models[model_to_train], str(model_to_train))\n",
    "\n",
    "else:\n",
    "    accuracy(load_models[model_to_train], str(model_to_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[model_to_train].eval()\n",
    "for i, (p, l, a) in enumerate(test_acc_loader):\n",
    "    p = p.to(device)\n",
    "    l = l.to(device)\n",
    "    # a = a.to(device)\n",
    "    models[model_to_train].eval()\n",
    "    output = models[model_to_train](p, l)\n",
    "    print(output)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[rev_out_vocab[x] for x in [ 1, 18,  7,  5,  4,  7,  8,  4,  5, 16, 18, 11, 12,  9, 13, 10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving vocabularies to be loaded for inference\n",
    "with open(out_dir + 'var/glove_vocab.json', 'w') as f:\n",
    "    json.dump(vocab, f)\n",
    "\n",
    "with open(out_dir + 'var/glove_out_vocab.json', 'w') as f:\n",
    "    json.dump(out_vocab, f)\n",
    "\n",
    "with open(out_dir + 'var/glove_rev_out_vocab.json', 'w') as f:\n",
    "    json.dump(rev_out_vocab, f)\n",
    "\n",
    "torch.save(embedding, out_dir + 'var/glove_embedding.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_o_v = json.load(open(out_dir + 'var/glove_rev_out_vocab.json', 'r'))\n",
    "r_o_v = {int(k):v for k,v in r_o_v.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
